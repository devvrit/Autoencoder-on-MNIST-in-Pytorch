{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b626a719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp                # JAX NumPy\n",
    "from jax import nn as jnn              # JAX nn\n",
    "# from jax.config import config\n",
    "# config.update('jax_enable_x64', True)\n",
    "from flax import linen as nn           # The Linen API\n",
    "from flax.training import train_state  # Useful dataclass to keep train state\n",
    "import time\n",
    "from absl import app\n",
    "from functools import partial\n",
    "from absl import flags\n",
    "import numpy as np                     # Ordinary NumPy\n",
    "import optax                           # Optimizers\n",
    "from keras.datasets import mnist\n",
    "from typing import List\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from typing import Any, List, NamedTuple, Callable, Optional, Union\n",
    "import jax.numpy as jnp\n",
    "from optax._src import utils\n",
    "from optax._src import combine\n",
    "from optax._src import base\n",
    "from optax._src import alias\n",
    "ScalarOrSchedule = Union[float, base.Schedule]\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75908539",
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_float('beta1', 0.9, help='Beta1')\n",
    "flags.DEFINE_float('beta2', 0.999, help='Beta2')\n",
    "flags.DEFINE_float('lr', 0.0001, help='Learning rate')\n",
    "flags.DEFINE_float('eps', 1e-8, help='eps')\n",
    "flags.DEFINE_integer('batch_size',\n",
    "                     1000, help='Batch size.')\n",
    "flags.DEFINE_integer('model_size_multiplier',\n",
    "                     1, help='Multiply model size by a constant')\n",
    "flags.DEFINE_integer('model_depth_multiplier',\n",
    "                     1, help='Multiply model depth by a constant')\n",
    "flags.DEFINE_integer('warmup_epochs', 5, help='Warmup epochs')\n",
    "flags.DEFINE_integer('epochs', 100, help='#Epochs')\n",
    "flags.DEFINE_integer('t', 20, help='preconditioner computation frequency')\n",
    "flags.DEFINE_enum('dtype', 'float32', ['float32', 'bfloat16'], help='dtype')\n",
    "flags.DEFINE_enum('optimizer', 'tds', ['sgd', 'momentum', 'nesterov', 'adagrad',\n",
    "  'rmsprop', 'tds', 'shampoo', 'diag_sonew'], help='optimizer')\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e6c6a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-19 17:00:02.857850: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-10-19 17:00:02.857897: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from absl import app\n",
    "\n",
    "# Addresses `UnrecognizedFlagError: Unknown command line flag 'f'`\n",
    "sys.argv = sys.argv[:1]\n",
    "\n",
    "# `app.run` calls `sys.exit`\n",
    "try:\n",
    "  app.run(lambda argv: None)\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc6f5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite FLAGS here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08a2bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "MACHINE_EPS = 1e-7 if FLAGS.dtype=='float32' else 0.0078"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9d276a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_edges_removed_tds = 0\n",
    "\n",
    "def ldl2tridiag(Lsub,D):\n",
    "  # n = D.shape[0]\n",
    "  Xd = jnp.zeros_like(D)\n",
    "  Xd = Xd.at[1:].set(D[1:]+Lsub*Lsub*D[:-1])\n",
    "  Xd = Xd.at[0].set(D[0])\n",
    "  Xe = Lsub*D[:-1]\n",
    "  return Xd,Xe\n",
    "\n",
    "def tridiagKFAC(Sd,Se, eps):\n",
    "  # given diagonal-Sd and subdiagonal-Se\n",
    "  # find the inverse of pd completion of this tridiagonal matrix\n",
    "  # interms of Ldiag(D)L^T decomposition\n",
    "  # outputs Lsub and D, where Lsub-subdiagonal of L\n",
    "  Sd = Sd+eps\n",
    "  psi = Se/Sd[1:]\n",
    "  condCov = jnp.zeros_like(Sd)\n",
    "  condCov = condCov.at[:-1].set(Sd[:-1]-Se*(Se/Sd[1:]))\n",
    "  condCov = condCov.at[-1].set(Sd[-1])\n",
    "  D = 1/(condCov)\n",
    "  mask1 = condCov[:-1]<=MACHINE_EPS*Sd[:-1]\n",
    "  mask2 = condCov <=MACHINE_EPS*Sd\n",
    "  psi = jnp.where(mask1, 0, psi)\n",
    "  D = jnp.where(mask2, 1/Sd, D)\n",
    "  Lsub = -psi\n",
    "  return ldl2tridiag(Lsub,D)\n",
    "\n",
    "def logdet_tds(Sd,Se, eps):\n",
    "  Sd = Sd+eps\n",
    "  psi = Se/Sd[1:]\n",
    "  condCov = jnp.zeros_like(Sd)\n",
    "  condCov = condCov.at[:-1].set(Sd[:-1]-Se*(Se/Sd[1:]))\n",
    "  condCov = condCov.at[-1].set(Sd[-1])\n",
    "  D = 1/(condCov)\n",
    "  mask1 = condCov[:-1]<=MACHINE_EPS*Sd[:-1]\n",
    "  mask2 = condCov <=MACHINE_EPS*Sd\n",
    "  psi = jnp.where(mask1, 0, psi)\n",
    "  D = jnp.where(mask2, 1/Sd, D)\n",
    "  return (-1*jnp.sum(jnp.log(D.astype(jnp.float32))), jnp.sum(mask2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ab32df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Sparse preconditioners.\"\"\"\n",
    "from typing import NamedTuple, Union\n",
    "\n",
    "import chex\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "ScalarOrSchedule = Union[float, optax.Schedule]\n",
    "\n",
    "\n",
    "def _update_moment(updates, moments, decay, order):\n",
    "  \"\"\"Compute the exponential moving average of the `order-th` moment.\"\"\"\n",
    "  return jax.tree_map(lambda g, t: (1 - decay) * (g**order) + decay * t,\n",
    "                           updates, moments)\n",
    "\n",
    "\n",
    "def _bias_correction(moment, decay, count):\n",
    "  \"\"\"Perform bias correction. This becomes a no-op as count goes to infinity.\"\"\"\n",
    "  beta = 1 - decay**count\n",
    "  return jax.tree_map(lambda t: t / beta.astype(t.dtype), moment)\n",
    "\n",
    "\n",
    "def scale_by_learning_rate(\n",
    "    learning_rate: ScalarOrSchedule,\n",
    "    flip_sign: bool = True) -> optax.GradientTransformation:\n",
    "  m = -1 if flip_sign else 1\n",
    "  if callable(learning_rate):\n",
    "    return optax.scale_by_schedule(lambda count: m * learning_rate(count))\n",
    "  return optax.scale(m * learning_rate)\n",
    "\n",
    "\n",
    "def _update_nu(updates, nu_e, nu_d, beta2):\n",
    "  \"\"\"Compute the exponential moving average of the tridiagonal structure of the moment.\"\"\"\n",
    "  nu_d = jax.tree_map(lambda g, t: (1 - beta2) * (g**2) + beta2 * t,\n",
    "                           updates, nu_d)\n",
    "  nu_e = jax.tree_map(\n",
    "      lambda g, t: (1 - beta2) * (g[:-1] * g[1:]) + beta2 * t, updates, nu_e)\n",
    "  return nu_e, nu_d\n",
    "\n",
    "\n",
    "class PreconditionTriDiagonalState(NamedTuple):\n",
    "  \"\"\"State for the Adam preconditioner.\"\"\"\n",
    "  count: chex.Array  # shape=(), dtype=jnp.int32\n",
    "  mu: optax.Updates\n",
    "  nu_e: optax.Updates\n",
    "  nu_d: optax.Updates\n",
    "  logdet: optax.Updates\n",
    "  num_edges_removed: optax.Updates\n",
    "\n",
    "def precondition_by_tds(\n",
    "    b1: float = 0.9,\n",
    "    b2: float = 0.999,\n",
    "    eps: float = 1e-8,\n",
    "    transpose: bool = True,\n",
    "    adam_grafting: bool = False,\n",
    "    debias: bool = True) -> optax.GradientTransformation:\n",
    "\n",
    "  def init_fn(params):\n",
    "    return PreconditionTriDiagonalState(\n",
    "        count=jnp.zeros([], jnp.int32),\n",
    "        mu = jax.tree_map(jnp.zeros_like, params),\n",
    "        logdet = jax.tree_map(jnp.zeros_like, params),\n",
    "        num_edges_removed = jax.tree_map(lambda g: jnp.array([0]), params),\n",
    "        nu_e=jax.tree_map(lambda g: jnp.zeros(len(g.reshape(-1))-1, dtype=g.dtype), params),\n",
    "        nu_d=jax.tree_map(lambda g: jnp.zeros(len(g.reshape(-1)), dtype=g.dtype), params))\n",
    "  @jax.jit\n",
    "  def update_fn(updates, state, params):\n",
    "    updates_hat = jax.tree_map(lambda g: g.T.reshape(-1) if transpose else g.reshape(-1), updates)\n",
    "    mu = _update_moment(updates, state.mu, b1, 1)\n",
    "    nu_e, nu_d = _update_nu(updates_hat, state.nu_e, state.nu_d, b2)\n",
    "    count = state.count + jnp.array(1, dtype=jnp.int32)\n",
    "    mu_hat = mu if not debias else _bias_correction(mu, b1, count)\n",
    "    nu_hat_e = nu_e if not debias else _bias_correction(nu_e, b2, count)\n",
    "    nu_hat_d = nu_d if not debias else _bias_correction(nu_d, b2, count)\n",
    "\n",
    "    temp = jax.tree_map(lambda d, e:\n",
    "                             tridiagKFAC(d,e, eps),\n",
    "                             nu_hat_d, nu_hat_e)\n",
    "    pre_d = jax.tree_map(lambda h, g: g[0], nu_hat_d, temp)\n",
    "    pre_e = jax.tree_map(lambda h, g: g[1], nu_hat_d, temp)\n",
    "    temp = jax.tree_map(lambda d, e: logdet_tds(d,e, eps), nu_hat_d, nu_hat_e)\n",
    "    logdet = jax.tree_map(lambda h, g: g[0], nu_hat_d, temp)\n",
    "    num_edges_removed = state.num_edges_removed\n",
    "    num_edges_removed = jax.tree_map(lambda h, g, l: l.at[0].set(l[0]+g[1]), nu_hat_d, temp, num_edges_removed)\n",
    "    mu_hat_flat = jax.tree_map(lambda m: m.T.reshape(-1)\n",
    "                                    if transpose else m.reshape(-1), mu_hat)\n",
    "    # Multiply gradient with diagonal\n",
    "    updates = jax.tree_map(lambda m, a: m*a, mu_hat_flat, pre_d)\n",
    "    # updates[i] = updates[i] + gradient[i-1]*pre_e[i], for i>0\n",
    "    updates = jax.tree_map(lambda u, m, a: u.at[1:].set(u[1:]+m[:-1]*a),\n",
    "                                updates, mu_hat_flat, pre_e)\n",
    "    # updates[i] = updates[i] + gradient[i+1]*pre_e[i], for i<n-1\n",
    "    updates = jax.tree_map(lambda u, m, a: u.at[:-1].set(u[:-1]+m[1:]*a),\n",
    "                                updates, mu_hat_flat, pre_e)\n",
    "    # reshape them to the original param shapes\n",
    "    updates = jax.tree_map(lambda mf, m: mf.reshape(m.T.shape).T\n",
    "                                if transpose else mf.reshape(m.shape),\n",
    "                                updates, mu_hat)\n",
    "    return updates, PreconditionTriDiagonalState(count=count, mu=mu, nu_e=nu_e,\n",
    "                                                 nu_d=nu_d, logdet=logdet,\n",
    "                                                 num_edges_removed=num_edges_removed)\n",
    "\n",
    "  return optax.GradientTransformation(init_fn, update_fn)\n",
    "\n",
    "def tds(learning_rate: ScalarOrSchedule, b1=0.9, b2=0.99, eps=1e-8, transpose=True, adam_grafting=False):\n",
    "    return combine.chain(\n",
    "      precondition_by_tds(\n",
    "          b1=b1, b2=b2, eps=eps, transpose=transpose, adam_grafting=adam_grafting),\n",
    "        alias._scale_by_learning_rate(learning_rate),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4e124a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_edges_removed_bds = 0\n",
    "\n",
    "def _update_nu_banded(updates, nu_e, nu_d, beta2):\n",
    "  nu_d = jax.tree_map(lambda g, t: (1-beta2) * (g**2) + beta2 * t,\n",
    "                      updates, nu_d)\n",
    "  def update_band(g, band, b):\n",
    "    for i in range(b):\n",
    "      band = band.at[:-(i+1), i].set((1-beta2)*(g[:-(i+1)]*g[i+1:]) + \n",
    "                                     beta2*band[:-(i+1), i])\n",
    "    return band\n",
    "  nu_e = jax.tree_map(lambda g, t: update_band(g, t, t.shape[-1]), updates,\n",
    "                      nu_e)\n",
    "  return nu_e, nu_d\n",
    "\n",
    "\n",
    "\n",
    "def GENP_jax(a, b):\n",
    "  \"\"\"Gaussian elimination with no pivoting.\n",
    "\n",
    "  % input: a is an batch x n x n nonsingular matrix\n",
    "  %        b is an batch x n x 1 vector\n",
    "  % return: x is the solution of Ax=b.\n",
    "  % post-condition: A and b have been modified. \n",
    "  \"\"\"\n",
    "  n = a.shape[1]\n",
    "  orig_a = a\n",
    "  if b.shape[1] != n:\n",
    "    raise ValueError(\"Invalid argument: \" +\n",
    "                     \"incompatible sizes between A & b.\", b.shape[-1], n)\n",
    "  for pivot_row in range(n-1):\n",
    "    for row in range(pivot_row+1, n):\n",
    "      den = a[:, pivot_row, pivot_row]\n",
    "      den = jnp.where(den == 0, MACHINE_EPS*orig_a[:, pivot_row, pivot_row], den)\n",
    "      a = a.at[:, pivot_row, pivot_row].set(den)\n",
    "      multiplier = a[:, row, pivot_row]/den\n",
    "      multiplier = multiplier.reshape(-1, 1)\n",
    "      a = a.at[:, row, pivot_row:].set(a[:, row, pivot_row:]-\n",
    "                                       multiplier*a[:, pivot_row, pivot_row:])\n",
    "      b = b.at[:, row].set(b[:, row] - multiplier*b[:, pivot_row])\n",
    "  batches = a.shape[0]\n",
    "  x = jnp.zeros((batches, n), dtype=a.dtype)\n",
    "  k = n-1\n",
    "  a = a.at[:, k, k].set(jnp.where(a[:, k, k] == 0, MACHINE_EPS*orig_a[:, k, k], a[:, k, k]))\n",
    "  den = a[:, k, k].reshape(-1, 1)\n",
    "  temp = b[:, k] / den\n",
    "  temp = temp.reshape(-1)\n",
    "  x = x.at[:, k].set(temp)\n",
    "  k = k-1\n",
    "  while k >= 0:\n",
    "    first = a[:, k, k+1:].reshape((batches, 1, -1))\n",
    "    second = x[:, k+1:].reshape((batches, -1, 1))\n",
    "    second_term = jnp.matmul(first, second)\n",
    "    temp = second_term.reshape(-1)\n",
    "    den = a[:, k, k].reshape(-1)\n",
    "    x = x.at[:, k].set((b[:, k].reshape(-1) - temp.reshape(-1))/den)\n",
    "    k = k-1\n",
    "  return x.reshape((batches, -1, 1))\n",
    "\n",
    "\n",
    "def bandedInv(Sd,subDiags,ind,eps,innerIters):\n",
    "  # given diagonal-Sd and subdiagonals-subDiags\n",
    "  # find the inverse of pd completion of this banded matrix\n",
    "  # interms of Ldiag(D)L^T decomposition\n",
    "  # outputs Lsub and D, where Lsub-subdiagonals of L\n",
    "\n",
    "  n = Sd.shape[0]\n",
    "  b = subDiags.shape[1]\n",
    "\n",
    "  bandvecs = jnp.concatenate((Sd.reshape(-1, 1), subDiags), axis=1)\n",
    "\n",
    "  indX,indY = ind\n",
    "  epsMat = jnp.zeros((b, b+1), dtype=Sd.dtype)\n",
    "  epsMat = epsMat.at[:,0].set(eps)\n",
    "  bandWindows = jnp.concatenate((bandvecs, epsMat), axis=0)\n",
    "  sig22 = bandWindows[indX[:,1:,1:],indY[:,1:,1:]]\n",
    "  sig21 = bandWindows[indX[:,1:,0],indY[:,1:,0]]\n",
    "\n",
    "  def A_bmm(X):\n",
    "    return jnp.matmul(sig22, X)\n",
    "\n",
    "  diagSig22 = jnp.diagonal(sig22, axis1=1, axis2=2)\n",
    "\n",
    "  def M_bmm(X):\n",
    "    return jnp.broadcast_to(jnp.expand_dims(1/diagSig22, axis=-1), X.shape)*X\n",
    "\n",
    "  # psi, _ = jax.scipy.sparse.linalg.cg(A_bmm, jnp.expand_dims(sig21, axis=-1),\n",
    "  #                                     tol=1e-8, M=M_bmm, maxiter=innerIters)\n",
    "  psi = GENP_jax(M_bmm(sig22), M_bmm(jnp.expand_dims(sig21, axis=-1)))\n",
    "  # psi = GENP_jax(sig22, jnp.expand_dims(sig21, axis=-1))\n",
    "  psi = psi.squeeze(-1)\n",
    "  # print(\"sig22:\\n\", sig22.reshape(-1))\n",
    "  # print(\"sig21:\", sig21.reshape(-1))\n",
    "  # print(\"psi:\", psi.reshape(-1))\n",
    "  # print(\"psi.shape:\", psi.shape)\n",
    "  # assert 1==2\n",
    "  # print(\"sig22:\", sig22)\n",
    "  # print(\"sig21:\", sig21)\n",
    "  # print(\"Sd:\", Sd)\n",
    "\n",
    "  psiSig21 = jnp.matmul(psi.reshape((n,1,b)),\n",
    "                        sig21.reshape((n, b, 1))).squeeze(-1).squeeze(-1)\n",
    "  condCov = Sd - psiSig21\n",
    "  # print(\"cond_cov:\", condCov)\n",
    "\n",
    "  ##################\n",
    "  '''\n",
    "  condCovFail = (condCov<=MACHINE_EPS*Sd).reshape((-1,1))\n",
    "  condCovFail = jnp.broadcast_to(condCovFail, (condCovFail.shape[0], b))\n",
    "  condCovFail = condCovFail.at[:-1,0].set(jnp.logical_or(condCovFail[:-1,0],\n",
    "                                                          condCovFail[1:,0]))\n",
    "  for i in range(1,b):\n",
    "    condCovFail = condCovFail.at[:-(i+1),i].set(\n",
    "        jnp.logical_or(condCovFail[:-(i+1), i], condCovFail[1:-(i), i-1]))\n",
    "  # global num_edges_removed\n",
    "  # num_edges_removed+=jnp.sum(condCovFail)\n",
    "  # print(\"jnp.sum(condCovFail)\", jnp.sum(condCovFail))\n",
    "\n",
    "  psi = jnp.where(condCovFail, 0.0, psi)\n",
    "  psiSig21 = jnp.matmul(psi.reshape((n, 1, b)), sig21.reshape((n, b, 1)))\n",
    "  psiSig21 = psiSig21.squeeze(-1).squeeze(-1)\n",
    "  condCov = Sd - psiSig21\n",
    "  '''\n",
    "  ####################\n",
    "\n",
    "  def cond(arguments):\n",
    "    condCov, psi, psiSig21, condCovFail = arguments\n",
    "    return jnp.any((condCov <= MACHINE_EPS*Sd))\n",
    "\n",
    "  def body(arguments):\n",
    "    condCov, psi, psiSig21, condCovFail = arguments\n",
    "    condCovFail = (condCov<=MACHINE_EPS*Sd).reshape((-1,1))\n",
    "    condCovFail = jnp.broadcast_to(condCovFail, (condCovFail.shape[0], b))\n",
    "    condCovFail = condCovFail.at[:-1,0].set(jnp.logical_or(condCovFail[:-1,0],\n",
    "                                                           condCovFail[1:,0]))\n",
    "    for i in range(1,b):\n",
    "      condCovFail = condCovFail.at[:-(i+1),i].set(\n",
    "          jnp.logical_or(condCovFail[:-(i+1), i], condCovFail[1:-(i), i-1]))\n",
    "\n",
    "    psi = jnp.where(condCovFail, 0.0, psi)\n",
    "    psiSig21 = jnp.matmul(psi.reshape((n, 1, b)), sig21.reshape((n, b, 1)))\n",
    "    psiSig21 = psiSig21.squeeze(-1).squeeze(-1)\n",
    "    condCov = Sd - psiSig21\n",
    "    return (condCov, psi, psiSig21, condCovFail)\n",
    "\n",
    "  condCovFail = (condCov<=MACHINE_EPS*Sd).reshape((-1,1))\n",
    "  condCovFail = jnp.broadcast_to(condCovFail, (condCovFail.shape[0], b))\n",
    "  condCovFail = condCovFail.at[:-1,0].set(jnp.logical_or(condCovFail[:-1,0], condCovFail[1:,0]))\n",
    "  ret = (condCov, psi, psiSig21, condCovFail)\n",
    "  ret = jax.lax.while_loop(cond, body, ret)\n",
    "  condCov, psi, psiSig21, condCovFail = ret\n",
    "  num_edges_removed_bds = jnp.sum(condCovFail)\n",
    "  D = 1/(condCov)\n",
    "  return psi.astype(Sd.dtype), D.astype(Sd.dtype), num_edges_removed_bds\n",
    "\n",
    "\n",
    "def bandedMult(psi, D, vecv):\n",
    "  b = psi.shape[1]\n",
    "  update = vecv\n",
    "  for i in range(b):\n",
    "    update = update.at[:-i-1].set(update[:-i-1] - vecv[i+1:]*psi[:-i-1, i])\n",
    "  update = update*D\n",
    "  vecv2 = update\n",
    "  for i in range(b):\n",
    "    update = update.at[i+1:].set(update[i+1:] - vecv2[:-i-1]*psi[:-i-1, i])\n",
    "  return update\n",
    "\n",
    "def getl1norm(Sd, Se):\n",
    "  bandSize = Se.shape[1]\n",
    "  n = Sd.shape[0]\n",
    "  temp = Sd\n",
    "  for b in range(bandSize):\n",
    "    temp = temp.at[:-(b+1)].set(Sd[:-(b+1)] + jnp.abs(Se[:, b][:-(b+1)]))\n",
    "    temp = temp.at[(b+1):].set(Sd[(b+1):] + jnp.abs(Se[:, b][:-(b+1)]))\n",
    "  return jnp.max(temp)\n",
    "\n",
    "def bandedUpdates(Sd, subDiags, ind, eps, innerIters, mu):\n",
    "  # innerIters = subDiags.shape[1]*13\n",
    "  l1norm = getl1norm(Sd, subDiags)\n",
    "  # psi, D = bandedInv(Sd+(eps*l1norm), subDiags, ind, eps, innerIters)\n",
    "  psi, D, num_edges_removed_bds = bandedInv(Sd+eps, subDiags, ind, eps, innerIters)\n",
    "  return (-1*jnp.sum(jnp.log(D.astype(jnp.float32))), num_edges_removed_bds)\n",
    "  # return bandedMult(psi, D, mu)\n",
    "\n",
    "def createInd(n,b):\n",
    "  b1 = b+1\n",
    "  offsetX = jnp.broadcast_to(jnp.expand_dims(jnp.arange(b1), axis=-1), (b1,b1))\n",
    "  offsetX = jnp.triu(offsetX)+jnp.transpose(jnp.triu(offsetX,1), (1,0))\n",
    "\n",
    "  offsetY = jnp.array(scipy.linalg.toeplitz(np.arange(b1)))\n",
    "\n",
    "  indX = jnp.broadcast_to(jnp.expand_dims(jnp.expand_dims(jnp.arange(n), axis=-1), axis=-1), (n,b1,b1))\n",
    "  indY = jnp.broadcast_to(jnp.expand_dims(jnp.expand_dims(jnp.zeros(n, dtype=jnp.int32), axis=-1), axis=-1), (n,b1,b1))\n",
    "\n",
    "  indX = indX+jnp.expand_dims(offsetX, 0)\n",
    "  indY = indY+jnp.expand_dims(offsetY, 0)\n",
    "\n",
    "  return jnp.array([indX, indY])\n",
    "\n",
    "class PreconditionBandedDiagonalState(NamedTuple):\n",
    "  \"\"\"State for the Adam preconditioner.\"\"\"\n",
    "  count: chex.Array  # shape=(), dtype=jnp.int32\n",
    "  mu: optax.Updates\n",
    "  nu_e: optax.Updates\n",
    "  nu_d: optax.Updates\n",
    "  ind: optax.Updates\n",
    "  diag: optax.Updates\n",
    "  logdet: optax.Updates\n",
    "  num_edges_removed: optax.Updates\n",
    "\n",
    "def precondition_by_bds(beta1: float = 0.9,\n",
    "                        beta2: float = 0.999,\n",
    "                        eps: float = 1e-8,\n",
    "                        graft_eps: float = 1e-8,\n",
    "                        graft_type: int = 0,\n",
    "                        transpose: bool = True,\n",
    "                        ridge_epsilon: float = 1e-12,\n",
    "                        b: int = 3,\n",
    "                        innerIters = 15,\n",
    "                        debias: bool = True) -> optax.GradientTransformation:\n",
    "  def init_fn(params):\n",
    "    diag = None\n",
    "    return PreconditionBandedDiagonalState(\n",
    "        count=jnp.zeros([], jnp.int32),\n",
    "        mu=jax.tree_map(jnp.zeros_like, params),\n",
    "        logdet=jax.tree_map(jnp.zeros_like, params),\n",
    "        num_edges_removed = jax.tree_map(lambda g: jnp.array([0]), params),\n",
    "        nu_e=jax.tree_map(lambda g: jnp.zeros((len(g.reshape(-1)), b),\n",
    "                                              dtype=g.dtype), params),\n",
    "        nu_d=jax.tree_map(lambda g: jnp.zeros(len(g.reshape(-1)),\n",
    "                                              dtype=g.dtype), params),\n",
    "        ind=jax.tree_map(lambda g: createInd(len(g.reshape(-1)), b), params),\n",
    "        diag=diag)\n",
    "  @jax.jit\n",
    "  def update_fn(updates, state, params):\n",
    "    del params\n",
    "    diag = state.diag\n",
    "    mu = state.mu\n",
    "    updates_hat = jax.tree_map(\n",
    "        lambda g: g.T.reshape(-1) if transpose else g.reshape(-1), updates)\n",
    "    nu_e, nu_d = _update_nu_banded(updates_hat, state.nu_e, state.nu_d, beta2)\n",
    "    count = state.count + jnp.array(1, dtype=jnp.int32)\n",
    "    mu_hat = mu if not debias else _bias_correction(mu, beta1, count)\n",
    "    nu_hat_e = nu_e if not debias else _bias_correction(nu_e, beta2, count)\n",
    "    nu_hat_d = nu_d if not debias else _bias_correction(nu_d, beta2, count)\n",
    "\n",
    "    mu_hat_flat = jax.tree_map(lambda m: m.T.reshape(-1)\n",
    "                               if transpose else m.reshape(-1), mu_hat)\n",
    "    temp = jax.tree_map(lambda d, e, g, ind: bandedUpdates(d, e, ind, eps,\n",
    "                                                             innerIters, g),\n",
    "                           nu_hat_d, nu_hat_e, mu_hat_flat, state.ind)\n",
    "    logdet = jax.tree_map(lambda h, g: g[0], nu_hat_d, temp)\n",
    "    num_edges_removed = state.num_edges_removed\n",
    "    num_edges_removed = jax.tree_map(lambda h, g, l: l.at[0].set(l[0]+g[1]), nu_hat_d, temp, num_edges_removed)\n",
    "    \n",
    "    return updates, PreconditionBandedDiagonalState(\n",
    "        count=count, mu=mu, nu_e=nu_e, nu_d=nu_d, ind=state.ind, diag=diag,\n",
    "        logdet=logdet, num_edges_removed=num_edges_removed)\n",
    "\n",
    "  return optax.GradientTransformation(init_fn, update_fn)\n",
    "\n",
    "\n",
    "def bds(learning_rate: ScalarOrSchedule,\n",
    "        beta1: float = 0.9,\n",
    "        beta2: float = 0.99,\n",
    "        eps: float = 1e-8,\n",
    "        graft_eps: float = 1e-8,\n",
    "        graft_type: int = 0,\n",
    "        weight_decay: float = 0.0,\n",
    "        ridge_epsilon: float = 1e-12,\n",
    "        b: int = 3,\n",
    "        transpose: bool = True) -> optax.GradientTransformation:\n",
    "  return optax.chain(\n",
    "      precondition_by_bds(beta1=beta1, beta2=beta2, eps=eps,\n",
    "                          graft_type=graft_type, innerIters=20,\n",
    "                          graft_eps=graft_eps, ridge_epsilon=ridge_epsilon, b=b,\n",
    "                          transpose=transpose),\n",
    "      scale_by_learning_rate(learning_rate),\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f61ff8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58debf67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dae74dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "  enc_hidden_states: List[int]\n",
    "  dec_hidden_states: List[int]\n",
    "  dtype: Any\n",
    "  param_dtype: Any\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    for i in range(len(self.enc_hidden_states)):\n",
    "      x = nn.Dense(features = self.enc_hidden_states[i],\n",
    "                   kernel_init=jnn.initializers.glorot_uniform(),\n",
    "                   dtype=self.dtype, param_dtype=self.param_dtype)(x)\n",
    "      if i<len(self.enc_hidden_states)-1:\n",
    "        x = nn.tanh(x)\n",
    "    for i in range(len(self.dec_hidden_states)):\n",
    "      x = nn.Dense(features = self.dec_hidden_states[i],\n",
    "                   kernel_init=jnn.initializers.glorot_uniform(),\n",
    "                   dtype=self.dtype, param_dtype=self.param_dtype)(x)\n",
    "      x = nn.tanh(x)\n",
    "    x = nn.Dense(features = 784,\n",
    "                 kernel_init=jnn.initializers.glorot_uniform(),\n",
    "                 dtype=self.dtype, param_dtype=self.param_dtype)(x)\n",
    "    return x\n",
    "\n",
    "  def __hash__(self):\n",
    "    return id(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55a294e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(opt, learning_rate):\n",
    "  print(\"using tds optimizer to generate gradients\")\n",
    "  return tds(learning_rate, b1=FLAGS.beta1, b2=FLAGS.beta2, eps=FLAGS.eps, transpose=True, adam_grafting=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f18fc8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_state(params, model, opt, learning_rate):\n",
    "  \"\"\"Creates initial `TrainState`.\"\"\"\n",
    "  tx = get_optimizer(opt, learning_rate)\n",
    "  return train_state.TrainState.create(\n",
    "      apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "@partial(jax.jit, static_argnums=0)\n",
    "def train_step(model, state, x):\n",
    "  def loss_fn(params):\n",
    "    logits = model.apply(params, x)\n",
    "    loss = optax.sigmoid_binary_cross_entropy(logits, x).mean(0).sum()\n",
    "    return loss\n",
    "  grad_fn = jax.value_and_grad(loss_fn)\n",
    "  loss, grads = grad_fn(state.params)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  return state, loss, grads\n",
    "\n",
    "@partial(jax.jit, static_argnums=0)\n",
    "def eval_step(model, state, x):\n",
    "  logits = model.apply(state.params, x)\n",
    "  loss = optax.sigmoid_binary_cross_entropy(logits, x)\n",
    "  return loss.astype(jnp.float32).mean(0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73f5e2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(state, model, train_ds, batch_size, epoch, rng, lrVec):\n",
    "  train_ds_size = len(train_ds)\n",
    "  steps_per_epoch = train_ds_size // batch_size\n",
    "  print(\"epoch:\", epoch,\"and lr going to be used:\", lrVec[epoch])\n",
    "  grads_list = []\n",
    "\n",
    "  perms = jax.random.permutation(rng, train_ds_size)\n",
    "  perms = perms[:steps_per_epoch * batch_size]  # skip incomplete batch\n",
    "  perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "  batch_metrics = []\n",
    "  for perm in perms:\n",
    "    train_x = train_ds[perm]\n",
    "    state, loss, grads = train_step(model, state, train_x)\n",
    "    # print(\"loss:\", loss, loss.dtype)\n",
    "    batch_metrics.append(loss.item())\n",
    "    grads_list.append(grads)\n",
    "\n",
    "  batch_metrics_np = jax.device_get(batch_metrics)\n",
    "  epoch_metrics_np = np.mean(batch_metrics_np)\n",
    "\n",
    "  # print('train epoch: %d, loss: %.4f' % (epoch, epoch_metrics_np))\n",
    "\n",
    "  return state, grads_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32991553",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_grads_list = []\n",
    "def main(argv):\n",
    "  train_start = time.time()\n",
    "  #Get random keys\n",
    "  rng = jax.random.PRNGKey(0)\n",
    "  rng, key1 = jax.random.split(rng)\n",
    "  rng, key2 = jax.random.split(rng)\n",
    "  rng, key3 = jax.random.split(rng)\n",
    "\n",
    "  #Get dtype\n",
    "  if FLAGS.dtype==\"float32\":\n",
    "    dtype = jnp.float32\n",
    "  elif FLAGS.dtype==\"bfloat16\":\n",
    "    dtype = jnp.bfloat16\n",
    "  else:\n",
    "      raise NotImplementedError\n",
    "\n",
    "  print(\"dtype is:\", dtype)\n",
    "  #Generate data\n",
    "  (train_inputs, _), (test_inputs, test_labels) = mnist.load_data()\n",
    "  train_inputs = jnp.array(train_inputs).astype(jnp.float32)\n",
    "  test_inputs = jnp.array(test_inputs).astype(jnp.float32)\n",
    "\n",
    "  # Rescale input images to [0, 1]\n",
    "  train_inputs = jnp.reshape(train_inputs, [-1, 784]) / 255.0\n",
    "  test_inputs = jnp.reshape(test_inputs, [-1, 784]) / 255.0\n",
    "\n",
    "  train_inputs = train_inputs.astype(dtype)\n",
    "  test_inputs = test_inputs.astype(dtype)\n",
    "\n",
    "  num_train_examples = train_inputs.shape[0]\n",
    "  num_test_examples = test_inputs.shape[0]\n",
    "  print('MNIST dataset:')\n",
    "  print('Num train examples: ' + str(num_train_examples))\n",
    "  print('Num test examples: ' + str(num_test_examples))\n",
    "\n",
    "  batch_size = FLAGS.batch_size\n",
    "\n",
    "  encoder_sizes = [1000] +  [500] * FLAGS.model_depth_multiplier + [250, 30]\n",
    "  decoder_sizes = [250] +  [500] * FLAGS.model_depth_multiplier + [1000]\n",
    "\n",
    "  encoder_sizes = [FLAGS.model_size_multiplier * e for e in encoder_sizes]\n",
    "  decoder_sizes = [FLAGS.model_size_multiplier * e for e in decoder_sizes]\n",
    "  encoder_decoder_sizes = encoder_sizes, decoder_sizes\n",
    "\n",
    "  input_image_batch = np.random.normal(size=(batch_size,784))\n",
    "  input_image_batch = jnp.array(input_image_batch).astype(dtype)\n",
    "\n",
    "  #Set learning rate schedule array\n",
    "  num_epochs = FLAGS.epochs\n",
    "  warmup_epochs = FLAGS.warmup_epochs\n",
    "  lr = FLAGS.lr\n",
    "  lrVec = np.concatenate([np.linspace(0,lr,warmup_epochs),\n",
    "                          np.linspace(lr,0,num_epochs-warmup_epochs+2)[1:-1]],\n",
    "                         axis=0)\n",
    "  lrVec = jnp.array(lrVec).astype(dtype)\n",
    "  def autoencoder_shedule(lrVec):\n",
    "    def schedule(count):\n",
    "      bucket = count//60\n",
    "      return lrVec[bucket]\n",
    "    return schedule\n",
    "\n",
    "  train_loss_val_=[]\n",
    "  model = Autoencoder(encoder_sizes, decoder_sizes, dtype=dtype, param_dtype=dtype)\n",
    "  params = model.init(key3, input_image_batch)\n",
    "  state = create_train_state(params, model, FLAGS.optimizer, autoencoder_shedule(lrVec))\n",
    "  print(\"Initialized model and optimizer!\")\n",
    "  global global_grads_list\n",
    "  for i in range(num_epochs):\n",
    "    rng, key = jax.random.split(rng)\n",
    "    epoch_start = time.time()\n",
    "    state, grads = train_epoch(state, model, train_inputs, FLAGS.batch_size, i, key, lrVec)\n",
    "    print(\"this epoch time:\", time.time()-epoch_start)\n",
    "    global_grads_list += grads\n",
    "    train_loss_val = eval_step(model, state, train_inputs)\n",
    "    train_loss_val_.append(train_loss_val)\n",
    "    print(\"epoch: \" + str(i) +\", train_loss_val: \" + str(train_loss_val))\n",
    "    print(\"\")\n",
    "  print(\"training time:\", time.time()-train_start)\n",
    "#   return global_grads_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3616eb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1019 17:00:34.867702 140604106622784 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: \n",
      "I1019 17:00:36.429756 140604106622784 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: Host Interpreter CUDA\n",
      "I1019 17:00:36.431886 140604106622784 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype is: <class 'jax.numpy.float32'>\n",
      "MNIST dataset:\n",
      "Num train examples: 60000\n",
      "Num test examples: 10000\n",
      "using tds optimizer to generate gradients\n",
      "Initialized model and optimizer!\n",
      "epoch: 0 and lr going to be used: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devvrit/anaconda3/envs/env/lib/python3.7/site-packages/flax/core/scope.py:740: FutureWarning: jax.tree_leaves is deprecated, and will be removed in a future release. Use jax.tree_util.tree_leaves instead.\n",
      "  abs_value_flat = jax.tree_leaves(abs_value)\n",
      "/home/devvrit/anaconda3/envs/env/lib/python3.7/site-packages/flax/core/scope.py:741: FutureWarning: jax.tree_leaves is deprecated, and will be removed in a future release. Use jax.tree_util.tree_leaves instead.\n",
      "  value_flat = jax.tree_leaves(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch time: 16.74345064163208\n",
      "epoch: 0, train_loss_val: 544.74426\n",
      "\n",
      "epoch: 1 and lr going to be used: 2.5e-05\n",
      "this epoch time: 0.2474362850189209\n",
      "epoch: 1, train_loss_val: 90.72567\n",
      "\n",
      "epoch: 2 and lr going to be used: 5e-05\n",
      "this epoch time: 0.2123701572418213\n",
      "epoch: 2, train_loss_val: 75.64782\n",
      "\n",
      "epoch: 3 and lr going to be used: 7.5e-05\n",
      "this epoch time: 0.21440744400024414\n",
      "epoch: 3, train_loss_val: 72.10297\n",
      "\n",
      "epoch: 4 and lr going to be used: 1e-04\n",
      "this epoch time: 0.21770763397216797\n",
      "epoch: 4, train_loss_val: 71.94248\n",
      "\n",
      "epoch: 5 and lr going to be used: 9.8958335e-05\n",
      "this epoch time: 0.21609210968017578\n",
      "epoch: 5, train_loss_val: 69.91649\n",
      "\n",
      "epoch: 6 and lr going to be used: 9.7916665e-05\n",
      "this epoch time: 0.21682119369506836\n",
      "epoch: 6, train_loss_val: 103.98162\n",
      "\n",
      "epoch: 7 and lr going to be used: 9.6875e-05\n",
      "this epoch time: 0.23123979568481445\n",
      "epoch: 7, train_loss_val: 78.250496\n",
      "\n",
      "epoch: 8 and lr going to be used: 9.583333e-05\n",
      "this epoch time: 0.23268604278564453\n",
      "epoch: 8, train_loss_val: 72.26166\n",
      "\n",
      "epoch: 9 and lr going to be used: 9.479166e-05\n",
      "this epoch time: 0.22933173179626465\n",
      "epoch: 9, train_loss_val: 69.51268\n",
      "\n",
      "epoch: 10 and lr going to be used: 9.375e-05\n",
      "this epoch time: 0.2292470932006836\n",
      "epoch: 10, train_loss_val: 67.774155\n",
      "\n",
      "epoch: 11 and lr going to be used: 9.270833e-05\n",
      "this epoch time: 0.2318432331085205\n",
      "epoch: 11, train_loss_val: 66.54777\n",
      "\n",
      "epoch: 12 and lr going to be used: 9.166667e-05\n",
      "this epoch time: 0.23198843002319336\n",
      "epoch: 12, train_loss_val: 65.5889\n",
      "\n",
      "epoch: 13 and lr going to be used: 9.0625e-05\n",
      "this epoch time: 0.20666933059692383\n",
      "epoch: 13, train_loss_val: 65.06241\n",
      "\n",
      "epoch: 14 and lr going to be used: 8.958334e-05\n",
      "this epoch time: 0.21412110328674316\n",
      "epoch: 14, train_loss_val: 64.41023\n",
      "\n",
      "epoch: 15 and lr going to be used: 8.854167e-05\n",
      "this epoch time: 0.21299362182617188\n",
      "epoch: 15, train_loss_val: 64.06841\n",
      "\n",
      "epoch: 16 and lr going to be used: 8.75e-05\n",
      "this epoch time: 0.22967767715454102\n",
      "epoch: 16, train_loss_val: 63.718502\n",
      "\n",
      "epoch: 17 and lr going to be used: 8.6458334e-05\n",
      "this epoch time: 0.23041176795959473\n",
      "epoch: 17, train_loss_val: 63.44061\n",
      "\n",
      "epoch: 18 and lr going to be used: 8.5416665e-05\n",
      "this epoch time: 0.23563909530639648\n",
      "epoch: 18, train_loss_val: 63.08678\n",
      "\n",
      "epoch: 19 and lr going to be used: 8.4375e-05\n",
      "this epoch time: 0.24898552894592285\n",
      "epoch: 19, train_loss_val: 62.94146\n",
      "\n",
      "epoch: 20 and lr going to be used: 8.333333e-05\n",
      "this epoch time: 0.2464275360107422\n",
      "epoch: 20, train_loss_val: 62.83183\n",
      "\n",
      "epoch: 21 and lr going to be used: 8.229167e-05\n",
      "this epoch time: 0.24686741828918457\n",
      "epoch: 21, train_loss_val: 62.557762\n",
      "\n",
      "epoch: 22 and lr going to be used: 8.125e-05\n",
      "this epoch time: 0.24074029922485352\n",
      "epoch: 22, train_loss_val: 62.552948\n",
      "\n",
      "epoch: 23 and lr going to be used: 8.020833e-05\n",
      "this epoch time: 0.23985886573791504\n",
      "epoch: 23, train_loss_val: 62.370007\n",
      "\n",
      "epoch: 24 and lr going to be used: 7.916667e-05\n",
      "this epoch time: 0.24385428428649902\n",
      "epoch: 24, train_loss_val: 62.34433\n",
      "\n",
      "epoch: 25 and lr going to be used: 7.8125e-05\n",
      "this epoch time: 0.24561858177185059\n",
      "epoch: 25, train_loss_val: 62.04956\n",
      "\n",
      "epoch: 26 and lr going to be used: 7.7083336e-05\n",
      "this epoch time: 0.23089075088500977\n",
      "epoch: 26, train_loss_val: 61.99212\n",
      "\n",
      "epoch: 27 and lr going to be used: 7.6041666e-05\n",
      "this epoch time: 0.20455598831176758\n",
      "epoch: 27, train_loss_val: 61.803795\n",
      "\n",
      "epoch: 28 and lr going to be used: 7.5e-05\n",
      "this epoch time: 0.2122020721435547\n",
      "epoch: 28, train_loss_val: 61.721428\n",
      "\n",
      "epoch: 29 and lr going to be used: 7.3958334e-05\n",
      "this epoch time: 0.21226000785827637\n",
      "epoch: 29, train_loss_val: 61.733196\n",
      "\n",
      "epoch: 30 and lr going to be used: 7.2916664e-05\n",
      "this epoch time: 0.21799564361572266\n",
      "epoch: 30, train_loss_val: 61.77513\n",
      "\n",
      "epoch: 31 and lr going to be used: 7.1875e-05\n",
      "this epoch time: 0.22808456420898438\n",
      "epoch: 31, train_loss_val: 61.52378\n",
      "\n",
      "epoch: 32 and lr going to be used: 7.083333e-05\n",
      "this epoch time: 0.22865056991577148\n",
      "epoch: 32, train_loss_val: 61.632324\n",
      "\n",
      "epoch: 33 and lr going to be used: 6.979167e-05\n",
      "this epoch time: 0.23492956161499023\n",
      "epoch: 33, train_loss_val: 61.49595\n",
      "\n",
      "epoch: 34 and lr going to be used: 6.875e-05\n",
      "this epoch time: 0.23247432708740234\n",
      "epoch: 34, train_loss_val: 61.37791\n",
      "\n",
      "epoch: 35 and lr going to be used: 6.770833e-05\n",
      "this epoch time: 0.2362837791442871\n",
      "epoch: 35, train_loss_val: 61.296246\n",
      "\n",
      "epoch: 36 and lr going to be used: 6.666667e-05\n",
      "this epoch time: 0.237379789352417\n",
      "epoch: 36, train_loss_val: 61.40262\n",
      "\n",
      "epoch: 37 and lr going to be used: 6.5625e-05\n",
      "this epoch time: 0.23750042915344238\n",
      "epoch: 37, train_loss_val: 61.111862\n",
      "\n",
      "epoch: 38 and lr going to be used: 6.4583335e-05\n",
      "this epoch time: 0.23450088500976562\n",
      "epoch: 38, train_loss_val: 61.279755\n",
      "\n",
      "epoch: 39 and lr going to be used: 6.3541665e-05\n",
      "this epoch time: 0.22915911674499512\n",
      "epoch: 39, train_loss_val: 61.127106\n",
      "\n",
      "epoch: 40 and lr going to be used: 6.25e-05\n",
      "this epoch time: 0.2174549102783203\n",
      "epoch: 40, train_loss_val: 61.125885\n",
      "\n",
      "epoch: 41 and lr going to be used: 6.145833e-05\n",
      "this epoch time: 0.23967671394348145\n",
      "epoch: 41, train_loss_val: 61.01812\n",
      "\n",
      "epoch: 42 and lr going to be used: 6.0416667e-05\n",
      "this epoch time: 0.24364852905273438\n",
      "epoch: 42, train_loss_val: 61.241417\n",
      "\n",
      "epoch: 43 and lr going to be used: 5.9375e-05\n",
      "this epoch time: 0.21806812286376953\n",
      "epoch: 43, train_loss_val: 60.748127\n",
      "\n",
      "epoch: 44 and lr going to be used: 5.8333335e-05\n",
      "this epoch time: 0.22129464149475098\n",
      "epoch: 44, train_loss_val: 60.820995\n",
      "\n",
      "epoch: 45 and lr going to be used: 5.7291665e-05\n",
      "this epoch time: 0.23673796653747559\n",
      "epoch: 45, train_loss_val: 60.897636\n",
      "\n",
      "epoch: 46 and lr going to be used: 5.625e-05\n",
      "this epoch time: 0.23132777214050293\n",
      "epoch: 46, train_loss_val: 60.62784\n",
      "\n",
      "epoch: 47 and lr going to be used: 5.5208333e-05\n",
      "this epoch time: 0.21414566040039062\n",
      "epoch: 47, train_loss_val: 60.61534\n",
      "\n",
      "epoch: 48 and lr going to be used: 5.4166667e-05\n",
      "this epoch time: 0.25327134132385254\n",
      "epoch: 48, train_loss_val: 60.41047\n",
      "\n",
      "epoch: 49 and lr going to be used: 5.3125e-05\n",
      "this epoch time: 0.2464594841003418\n",
      "epoch: 49, train_loss_val: 60.458725\n",
      "\n",
      "epoch: 50 and lr going to be used: 5.2083335e-05\n",
      "this epoch time: 0.222808837890625\n",
      "epoch: 50, train_loss_val: 60.264206\n",
      "\n",
      "epoch: 51 and lr going to be used: 5.104167e-05\n",
      "this epoch time: 0.2156214714050293\n",
      "epoch: 51, train_loss_val: 60.313103\n",
      "\n",
      "epoch: 52 and lr going to be used: 5e-05\n",
      "this epoch time: 0.22634148597717285\n",
      "epoch: 52, train_loss_val: 60.07862\n",
      "\n",
      "epoch: 53 and lr going to be used: 4.8958333e-05\n",
      "this epoch time: 0.22627592086791992\n",
      "epoch: 53, train_loss_val: 60.114235\n",
      "\n",
      "epoch: 54 and lr going to be used: 4.7916667e-05\n",
      "this epoch time: 0.2263031005859375\n",
      "epoch: 54, train_loss_val: 60.005848\n",
      "\n",
      "epoch: 55 and lr going to be used: 4.6875e-05\n",
      "this epoch time: 0.22133684158325195\n",
      "epoch: 55, train_loss_val: 60.064667\n",
      "\n",
      "epoch: 56 and lr going to be used: 4.5833334e-05\n",
      "this epoch time: 0.22856926918029785\n",
      "epoch: 56, train_loss_val: 59.76468\n",
      "\n",
      "epoch: 57 and lr going to be used: 4.479167e-05\n",
      "this epoch time: 0.2285442352294922\n",
      "epoch: 57, train_loss_val: 59.65532\n",
      "\n",
      "epoch: 58 and lr going to be used: 4.375e-05\n",
      "this epoch time: 0.23660063743591309\n",
      "epoch: 58, train_loss_val: 59.65845\n",
      "\n",
      "epoch: 59 and lr going to be used: 4.2708332e-05\n",
      "this epoch time: 0.2372443675994873\n",
      "epoch: 59, train_loss_val: 59.532837\n",
      "\n",
      "epoch: 60 and lr going to be used: 4.1666666e-05\n",
      "this epoch time: 0.24171066284179688\n",
      "epoch: 60, train_loss_val: 59.496\n",
      "\n",
      "epoch: 61 and lr going to be used: 4.0625e-05\n",
      "this epoch time: 0.23923754692077637\n",
      "epoch: 61, train_loss_val: 59.1913\n",
      "\n",
      "epoch: 62 and lr going to be used: 3.9583334e-05\n",
      "this epoch time: 0.22395873069763184\n",
      "epoch: 62, train_loss_val: 59.120724\n",
      "\n",
      "epoch: 63 and lr going to be used: 3.8541668e-05\n",
      "this epoch time: 0.23395967483520508\n",
      "epoch: 63, train_loss_val: 59.067688\n",
      "\n",
      "epoch: 64 and lr going to be used: 3.75e-05\n",
      "this epoch time: 0.23409628868103027\n",
      "epoch: 64, train_loss_val: 59.045364\n",
      "\n",
      "epoch: 65 and lr going to be used: 3.6458332e-05\n",
      "this epoch time: 0.22713136672973633\n",
      "epoch: 65, train_loss_val: 58.898846\n",
      "\n",
      "epoch: 66 and lr going to be used: 3.5416666e-05\n",
      "this epoch time: 0.19997501373291016\n",
      "epoch: 66, train_loss_val: 58.642494\n",
      "\n",
      "epoch: 67 and lr going to be used: 3.4375e-05\n",
      "this epoch time: 0.2041795253753662\n",
      "epoch: 67, train_loss_val: 58.449833\n",
      "\n",
      "epoch: 68 and lr going to be used: 3.3333334e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch time: 0.2052001953125\n",
      "epoch: 68, train_loss_val: 58.379463\n",
      "\n",
      "epoch: 69 and lr going to be used: 3.2291668e-05\n",
      "this epoch time: 0.20019030570983887\n",
      "epoch: 69, train_loss_val: 58.167492\n",
      "\n",
      "epoch: 70 and lr going to be used: 3.125e-05\n",
      "this epoch time: 0.2182753086090088\n",
      "epoch: 70, train_loss_val: 57.94629\n",
      "\n",
      "epoch: 71 and lr going to be used: 3.0208334e-05\n",
      "this epoch time: 0.224928617477417\n",
      "epoch: 71, train_loss_val: 57.91194\n",
      "\n",
      "epoch: 72 and lr going to be used: 2.9166667e-05\n",
      "this epoch time: 0.22822093963623047\n",
      "epoch: 72, train_loss_val: 57.74287\n",
      "\n",
      "epoch: 73 and lr going to be used: 2.8125e-05\n",
      "this epoch time: 0.23056745529174805\n",
      "epoch: 73, train_loss_val: 57.61487\n",
      "\n",
      "epoch: 74 and lr going to be used: 2.7083333e-05\n",
      "this epoch time: 0.21734929084777832\n",
      "epoch: 74, train_loss_val: 57.46028\n",
      "\n",
      "epoch: 75 and lr going to be used: 2.6041667e-05\n",
      "this epoch time: 0.22044610977172852\n",
      "epoch: 75, train_loss_val: 57.213398\n",
      "\n",
      "epoch: 76 and lr going to be used: 2.5e-05\n",
      "this epoch time: 0.21688270568847656\n",
      "epoch: 76, train_loss_val: 57.043404\n",
      "\n",
      "epoch: 77 and lr going to be used: 2.3958333e-05\n",
      "this epoch time: 0.22081232070922852\n",
      "epoch: 77, train_loss_val: 56.845413\n",
      "\n",
      "epoch: 78 and lr going to be used: 2.2916667e-05\n",
      "this epoch time: 0.21604228019714355\n",
      "epoch: 78, train_loss_val: 56.707176\n",
      "\n",
      "epoch: 79 and lr going to be used: 2.1875e-05\n",
      "this epoch time: 0.21904635429382324\n",
      "epoch: 79, train_loss_val: 56.5047\n",
      "\n",
      "epoch: 80 and lr going to be used: 2.0833333e-05\n",
      "this epoch time: 0.2172996997833252\n",
      "epoch: 80, train_loss_val: 56.291\n",
      "\n",
      "epoch: 81 and lr going to be used: 1.9791667e-05\n",
      "this epoch time: 0.21353435516357422\n",
      "epoch: 81, train_loss_val: 56.014122\n",
      "\n",
      "epoch: 82 and lr going to be used: 1.875e-05\n",
      "this epoch time: 0.20261120796203613\n",
      "epoch: 82, train_loss_val: 55.79657\n",
      "\n",
      "epoch: 83 and lr going to be used: 1.7708333e-05\n",
      "this epoch time: 0.20574641227722168\n",
      "epoch: 83, train_loss_val: 55.59513\n",
      "\n",
      "epoch: 84 and lr going to be used: 1.6666667e-05\n",
      "this epoch time: 0.20661282539367676\n",
      "epoch: 84, train_loss_val: 55.396606\n",
      "\n",
      "epoch: 85 and lr going to be used: 1.5625e-05\n",
      "this epoch time: 0.2131800651550293\n",
      "epoch: 85, train_loss_val: 55.21305\n",
      "\n",
      "epoch: 86 and lr going to be used: 1.4583334e-05\n",
      "this epoch time: 0.22179508209228516\n",
      "epoch: 86, train_loss_val: 54.976173\n",
      "\n",
      "epoch: 87 and lr going to be used: 1.3541667e-05\n",
      "this epoch time: 0.21298456192016602\n",
      "epoch: 87, train_loss_val: 54.764145\n",
      "\n",
      "epoch: 88 and lr going to be used: 1.25e-05\n",
      "this epoch time: 0.22501564025878906\n",
      "epoch: 88, train_loss_val: 54.58296\n",
      "\n",
      "epoch: 89 and lr going to be used: 1.1458334e-05\n",
      "this epoch time: 0.22943353652954102\n",
      "epoch: 89, train_loss_val: 54.364517\n",
      "\n",
      "epoch: 90 and lr going to be used: 1.0416667e-05\n",
      "this epoch time: 0.23428940773010254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-19 17:01:45.134419: W external/org_tensorflow/tensorflow/core/common_runtime/bfc_allocator.cc:479] Allocator (GPU_0_bfc) ran out of memory trying to allocate 408.33MiB (rounded to 428160000)requested by op \n",
      "2022-10-19 17:01:45.228100: W external/org_tensorflow/tensorflow/core/common_runtime/bfc_allocator.cc:491] ****************************************************************************************************\n",
      "2022-10-19 17:01:45.228488: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2129] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 428160000 bytes.\n",
      "BufferAssignment OOM Debugging.\n",
      "BufferAssignment stats:\n",
      "             parameter allocation:  190.27MiB\n",
      "              constant allocation:         4B\n",
      "        maybe_live_out allocation:         4B\n",
      "     preallocated temp allocation:  408.33MiB\n",
      "  preallocated temp fragmentation:         0B (0.00%)\n",
      "                 total allocation:  598.59MiB\n",
      "Peak buffers:\n",
      "\tBuffer 1:\n",
      "\t\tSize: 228.88MiB\n",
      "\t\tOperator: op_name=\"jit(eval_step)/jit(main)/Autoencoder/tanh\" source_file=\"/tmp/ipykernel_219857/3037788847.py\" source_line=19\n",
      "\t\tXLA Label: tanh\n",
      "\t\tShape: f32[60000,1000]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 2:\n",
      "\t\tSize: 179.44MiB\n",
      "\t\tOperator: op_name=\"jit(eval_step)/jit(main)/Autoencoder/Dense_7/add\" source_file=\"/home/devvrit/anaconda3/envs/env/lib/python3.7/site-packages/flax/linen/linear.py\" source_line=200\n",
      "\t\tXLA Label: broadcast\n",
      "\t\tShape: f32[60000,784]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 3:\n",
      "\t\tSize: 179.44MiB\n",
      "\t\tEntry Parameter Subshape: f32[60000,784]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 4:\n",
      "\t\tSize: 2.99MiB\n",
      "\t\tEntry Parameter Subshape: f32[1000,784]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 5:\n",
      "\t\tSize: 2.99MiB\n",
      "\t\tEntry Parameter Subshape: f32[784,1000]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 6:\n",
      "\t\tSize: 1.91MiB\n",
      "\t\tEntry Parameter Subshape: f32[500,1000]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 7:\n",
      "\t\tSize: 1.91MiB\n",
      "\t\tEntry Parameter Subshape: f32[1000,500]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 8:\n",
      "\t\tSize: 488.3KiB\n",
      "\t\tEntry Parameter Subshape: f32[250,500]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 9:\n",
      "\t\tSize: 488.3KiB\n",
      "\t\tEntry Parameter Subshape: f32[500,250]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 10:\n",
      "\t\tSize: 29.3KiB\n",
      "\t\tEntry Parameter Subshape: f32[30,250]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 11:\n",
      "\t\tSize: 29.3KiB\n",
      "\t\tEntry Parameter Subshape: f32[250,30]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 12:\n",
      "\t\tSize: 3.9KiB\n",
      "\t\tEntry Parameter Subshape: f32[1000]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 13:\n",
      "\t\tSize: 3.9KiB\n",
      "\t\tEntry Parameter Subshape: f32[1000]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 14:\n",
      "\t\tSize: 3.1KiB\n",
      "\t\tEntry Parameter Subshape: f32[784]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 15:\n",
      "\t\tSize: 2.0KiB\n",
      "\t\tEntry Parameter Subshape: f32[500]\n",
      "\t\t==========================\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "RESOURCE_EXHAUSTED: Out of memory while trying to allocate 428160000 bytes.\nBufferAssignment OOM Debugging.\nBufferAssignment stats:\n             parameter allocation:  190.27MiB\n              constant allocation:         4B\n        maybe_live_out allocation:         4B\n     preallocated temp allocation:  408.33MiB\n  preallocated temp fragmentation:         0B (0.00%)\n                 total allocation:  598.59MiB\nPeak buffers:\n\tBuffer 1:\n\t\tSize: 228.88MiB\n\t\tOperator: op_name=\"jit(eval_step)/jit(main)/Autoencoder/tanh\" source_file=\"/tmp/ipykernel_219857/3037788847.py\" source_line=19\n\t\tXLA Label: tanh\n\t\tShape: f32[60000,1000]\n\t\t==========================\n\n\tBuffer 2:\n\t\tSize: 179.44MiB\n\t\tOperator: op_name=\"jit(eval_step)/jit(main)/Autoencoder/Dense_7/add\" source_file=\"/home/devvrit/anaconda3/envs/env/lib/python3.7/site-packages/flax/linen/linear.py\" source_line=200\n\t\tXLA Label: broadcast\n\t\tShape: f32[60000,784]\n\t\t==========================\n\n\tBuffer 3:\n\t\tSize: 179.44MiB\n\t\tEntry Parameter Subshape: f32[60000,784]\n\t\t==========================\n\n\tBuffer 4:\n\t\tSize: 2.99MiB\n\t\tEntry Parameter Subshape: f32[1000,784]\n\t\t==========================\n\n\tBuffer 5:\n\t\tSize: 2.99MiB\n\t\tEntry Parameter Subshape: f32[784,1000]\n\t\t==========================\n\n\tBuffer 6:\n\t\tSize: 1.91MiB\n\t\tEntry Parameter Subshape: f32[500,1000]\n\t\t==========================\n\n\tBuffer 7:\n\t\tSize: 1.91MiB\n\t\tEntry Parameter Subshape: f32[1000,500]\n\t\t==========================\n\n\tBuffer 8:\n\t\tSize: 488.3KiB\n\t\tEntry Parameter Subshape: f32[250,500]\n\t\t==========================\n\n\tBuffer 9:\n\t\tSize: 488.3KiB\n\t\tEntry Parameter Subshape: f32[500,250]\n\t\t==========================\n\n\tBuffer 10:\n\t\tSize: 29.3KiB\n\t\tEntry Parameter Subshape: f32[30,250]\n\t\t==========================\n\n\tBuffer 11:\n\t\tSize: 29.3KiB\n\t\tEntry Parameter Subshape: f32[250,30]\n\t\t==========================\n\n\tBuffer 12:\n\t\tSize: 3.9KiB\n\t\tEntry Parameter Subshape: f32[1000]\n\t\t==========================\n\n\tBuffer 13:\n\t\tSize: 3.9KiB\n\t\tEntry Parameter Subshape: f32[1000]\n\t\t==========================\n\n\tBuffer 14:\n\t\tSize: 3.1KiB\n\t\tEntry Parameter Subshape: f32[784]\n\t\t==========================\n\n\tBuffer 15:\n\t\tSize: 2.0KiB\n\t\tEntry Parameter Subshape: f32[500]\n\t\t==========================\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_219857/1052693553.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m############# GET GRADIENTS USING tds OPTIMIZER ################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.7/site-packages/absl/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv, flags_parser)\u001b[0m\n\u001b[1;32m    310\u001b[0m       \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m       \u001b[0m_run_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mUsageError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0musage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshorthelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetailed_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexitcode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.7/site-packages/absl/app.py\u001b[0m in \u001b[0;36m_run_main\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_219857/2215490168.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(argv)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"this epoch time:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mepoch_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mglobal_grads_list\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mtrain_loss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0mtrain_loss_val_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\", train_loss_val: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 428160000 bytes.\nBufferAssignment OOM Debugging.\nBufferAssignment stats:\n             parameter allocation:  190.27MiB\n              constant allocation:         4B\n        maybe_live_out allocation:         4B\n     preallocated temp allocation:  408.33MiB\n  preallocated temp fragmentation:         0B (0.00%)\n                 total allocation:  598.59MiB\nPeak buffers:\n\tBuffer 1:\n\t\tSize: 228.88MiB\n\t\tOperator: op_name=\"jit(eval_step)/jit(main)/Autoencoder/tanh\" source_file=\"/tmp/ipykernel_219857/3037788847.py\" source_line=19\n\t\tXLA Label: tanh\n\t\tShape: f32[60000,1000]\n\t\t==========================\n\n\tBuffer 2:\n\t\tSize: 179.44MiB\n\t\tOperator: op_name=\"jit(eval_step)/jit(main)/Autoencoder/Dense_7/add\" source_file=\"/home/devvrit/anaconda3/envs/env/lib/python3.7/site-packages/flax/linen/linear.py\" source_line=200\n\t\tXLA Label: broadcast\n\t\tShape: f32[60000,784]\n\t\t==========================\n\n\tBuffer 3:\n\t\tSize: 179.44MiB\n\t\tEntry Parameter Subshape: f32[60000,784]\n\t\t==========================\n\n\tBuffer 4:\n\t\tSize: 2.99MiB\n\t\tEntry Parameter Subshape: f32[1000,784]\n\t\t==========================\n\n\tBuffer 5:\n\t\tSize: 2.99MiB\n\t\tEntry Parameter Subshape: f32[784,1000]\n\t\t==========================\n\n\tBuffer 6:\n\t\tSize: 1.91MiB\n\t\tEntry Parameter Subshape: f32[500,1000]\n\t\t==========================\n\n\tBuffer 7:\n\t\tSize: 1.91MiB\n\t\tEntry Parameter Subshape: f32[1000,500]\n\t\t==========================\n\n\tBuffer 8:\n\t\tSize: 488.3KiB\n\t\tEntry Parameter Subshape: f32[250,500]\n\t\t==========================\n\n\tBuffer 9:\n\t\tSize: 488.3KiB\n\t\tEntry Parameter Subshape: f32[500,250]\n\t\t==========================\n\n\tBuffer 10:\n\t\tSize: 29.3KiB\n\t\tEntry Parameter Subshape: f32[30,250]\n\t\t==========================\n\n\tBuffer 11:\n\t\tSize: 29.3KiB\n\t\tEntry Parameter Subshape: f32[250,30]\n\t\t==========================\n\n\tBuffer 12:\n\t\tSize: 3.9KiB\n\t\tEntry Parameter Subshape: f32[1000]\n\t\t==========================\n\n\tBuffer 13:\n\t\tSize: 3.9KiB\n\t\tEntry Parameter Subshape: f32[1000]\n\t\t==========================\n\n\tBuffer 14:\n\t\tSize: 3.1KiB\n\t\tEntry Parameter Subshape: f32[784]\n\t\t==========================\n\n\tBuffer 15:\n\t\tSize: 2.0KiB\n\t\tEntry Parameter Subshape: f32[500]\n\t\t==========================\n\n"
     ]
    }
   ],
   "source": [
    "############# GET GRADIENTS USING tds OPTIMIZER ################\n",
    "app.run(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59cd0127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5460"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(global_grads_list) # should be 60*FLAGS.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c618bb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5460it [02:29, 36.48it/s]\n"
     ]
    }
   ],
   "source": [
    "num_edges_removed_tds = 0\n",
    "optimizer_tds = tds(0.0, b1=FLAGS.beta1, b2=FLAGS.beta2, eps=FLAGS.eps, transpose=True, adam_grafting=False)\n",
    "tds_state = optimizer_tds.init(global_grads_list[0])\n",
    "log_det_tds = jax.tree_map(lambda g: jnp.zeros(len(global_grads_list)), global_grads_list[0])\n",
    "eps = FLAGS.eps\n",
    "for i, grads in tqdm(enumerate(global_grads_list)):\n",
    "  updates, new_opt_state = optimizer_tds.update(grads, tds_state, None)\n",
    "  log_det_tds = jax.tree_map(lambda l, v: l.at[i].set(v), log_det_tds, new_opt_state[0].logdet)\n",
    "  tds_state = new_opt_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffef8649",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-19 17:07:33.894007: W external/org_tensorflow/tensorflow/core/common_runtime/bfc_allocator.cc:479] Allocator (GPU_0_bfc) ran out of memory trying to allocate 149.54MiB (rounded to 156800000)requested by op \n",
      "2022-10-19 17:07:34.016604: W external/org_tensorflow/tensorflow/core/common_runtime/bfc_allocator.cc:491] ****************************************************************************************************\n",
      "2022-10-19 17:07:34.016765: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2129] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 156800000 bytes.\n",
      "BufferAssignment OOM Debugging.\n",
      "BufferAssignment stats:\n",
      "             parameter allocation:  149.54MiB\n",
      "              constant allocation:         0B\n",
      "        maybe_live_out allocation:  149.54MiB\n",
      "     preallocated temp allocation:         0B\n",
      "                 total allocation:  299.07MiB\n",
      "              total fragmentation:         0B (0.00%)\n",
      "Peak buffers:\n",
      "\tBuffer 1:\n",
      "\t\tSize: 149.54MiB\n",
      "\t\tOperator: op_name=\"jit(concatenate)/jit(main)/concatenate[dimension=0]\" source_file=\"/tmp/ipykernel_219857/543851162.py\" source_line=197\n",
      "\t\tXLA Label: concatenate\n",
      "\t\tShape: s32[2,784000,5,5]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 2:\n",
      "\t\tSize: 74.77MiB\n",
      "\t\tEntry Parameter Subshape: s32[1,784000,5,5]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 3:\n",
      "\t\tSize: 74.77MiB\n",
      "\t\tEntry Parameter Subshape: s32[1,784000,5,5]\n",
      "\t\t==========================\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "RESOURCE_EXHAUSTED: Out of memory while trying to allocate 156800000 bytes.\nBufferAssignment OOM Debugging.\nBufferAssignment stats:\n             parameter allocation:  149.54MiB\n              constant allocation:         0B\n        maybe_live_out allocation:  149.54MiB\n     preallocated temp allocation:         0B\n                 total allocation:  299.07MiB\n              total fragmentation:         0B (0.00%)\nPeak buffers:\n\tBuffer 1:\n\t\tSize: 149.54MiB\n\t\tOperator: op_name=\"jit(concatenate)/jit(main)/concatenate[dimension=0]\" source_file=\"/tmp/ipykernel_219857/543851162.py\" source_line=197\n\t\tXLA Label: concatenate\n\t\tShape: s32[2,784000,5,5]\n\t\t==========================\n\n\tBuffer 2:\n\t\tSize: 74.77MiB\n\t\tEntry Parameter Subshape: s32[1,784000,5,5]\n\t\t==========================\n\n\tBuffer 3:\n\t\tSize: 74.77MiB\n\t\tEntry Parameter Subshape: s32[1,784000,5,5]\n\t\t==========================\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_219857/911920126.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_edges_removed_bds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moptimizer_bds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraft_eps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraft_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbds_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer_bds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_grads_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mlog_det_bds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_grads_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_grads_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.7/site-packages/optax/_src/combine.py\u001b[0m in \u001b[0;36minit_fn\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0minit_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minit_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mupdate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.7/site-packages/optax/_src/combine.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0minit_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minit_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mupdate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_219857/543851162.py\u001b[0m in \u001b[0;36minit_fn\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    229\u001b[0m         nu_d=jax.tree_map(lambda g: jnp.zeros(len(g.reshape(-1)),\n\u001b[1;32m    230\u001b[0m                                               dtype=g.dtype), params),\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcreateInd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         diag=diag)\n\u001b[1;32m    233\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.7/site-packages/jax/_src/tree_util.py\u001b[0m in \u001b[0;36mtree_map\u001b[0;34m(f, tree, is_leaf, *rest)\u001b[0m\n\u001b[1;32m    203\u001b[0m   \u001b[0mleaves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreedef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m   \u001b[0mall_leaves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtreedef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_up_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtreedef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mall_leaves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtreedef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.7/site-packages/jax/_src/tree_util.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    203\u001b[0m   \u001b[0mleaves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreedef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m   \u001b[0mall_leaves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtreedef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_up_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtreedef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mall_leaves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtreedef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_219857/543851162.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(g)\u001b[0m\n\u001b[1;32m    229\u001b[0m         nu_d=jax.tree_map(lambda g: jnp.zeros(len(g.reshape(-1)),\n\u001b[1;32m    230\u001b[0m                                               dtype=g.dtype), params),\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcreateInd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         diag=diag)\n\u001b[1;32m    233\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_219857/543851162.py\u001b[0m in \u001b[0;36mcreateInd\u001b[0;34m(n, b)\u001b[0m\n\u001b[1;32m    195\u001b[0m   \u001b[0mindY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindY\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffsetY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindY\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mPreconditionBandedDiagonalState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNamedTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.7/site-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36marray\u001b[0;34m(object, dtype, copy, order, ndmin)\u001b[0m\n\u001b[1;32m   1887\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1888\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1889\u001b[0;31m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1890\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1891\u001b[0m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.7/site-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out)\u001b[0m\n\u001b[1;32m   1634\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All input arrays must have the same shape.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1635\u001b[0m       \u001b[0mnew_arrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1636\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_wraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.7/site-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(arrays, axis, dtype)\u001b[0m\n\u001b[1;32m   1692\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m       arrays = [lax.concatenate(arrays[i:i+k], axis)\n\u001b[0;32m-> 1694\u001b[0;31m                 for i in range(0, len(arrays), k)]\n\u001b[0m\u001b[1;32m   1695\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.7/site-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1692\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m       arrays = [lax.concatenate(arrays[i:i+k], axis)\n\u001b[0;32m-> 1694\u001b[0;31m                 for i in range(0, len(arrays), k)]\n\u001b[0m\u001b[1;32m   1695\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.7/site-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(operands, dimension)\u001b[0m\n\u001b[1;32m    625\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"concatenate requires a non-empty sequences of arrays\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconcatenate_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdimension\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.7/site-packages/jax/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    322\u001b[0m     assert (not config.jax_enable_checks or\n\u001b[1;32m    323\u001b[0m             all(isinstance(arg, Tracer) or valid_jaxtype(arg) for arg in args)), args\n\u001b[0;32m--> 324\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_top_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.7/site-packages/jax/core.py\u001b[0m in \u001b[0;36mbind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfull_lower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.7/site-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.7/site-packages/jax/_src/dispatch.py\u001b[0m in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m     99\u001b[0m   compiled_fun = xla_primitive_callable(prim, *unsafe_map(arg_spec, args),\n\u001b[1;32m    100\u001b[0m                                         **params)\n\u001b[0;32m--> 101\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m# TODO(phawkins): update code referring to xla.apply_primitive to point here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.7/site-packages/jax/_src/dispatch.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    174\u001b[0m                                     prim.name, donated_invars, False, *arg_specs)\n\u001b[1;32m    175\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mprim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.7/site-packages/jax/_src/dispatch.py\u001b[0m in \u001b[0;36m_execute_compiled\u001b[0;34m(name, compiled, input_handler, output_buffer_counts, result_handler, has_unordered_effects, ordered_effects, kept_var_idx, *args)\u001b[0m\n\u001b[1;32m    753\u001b[0m       \u001b[0mruntime_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m     \u001b[0mout_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m   \u001b[0mcheck_special\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m   \u001b[0mout_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_buffer_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 156800000 bytes.\nBufferAssignment OOM Debugging.\nBufferAssignment stats:\n             parameter allocation:  149.54MiB\n              constant allocation:         0B\n        maybe_live_out allocation:  149.54MiB\n     preallocated temp allocation:         0B\n                 total allocation:  299.07MiB\n              total fragmentation:         0B (0.00%)\nPeak buffers:\n\tBuffer 1:\n\t\tSize: 149.54MiB\n\t\tOperator: op_name=\"jit(concatenate)/jit(main)/concatenate[dimension=0]\" source_file=\"/tmp/ipykernel_219857/543851162.py\" source_line=197\n\t\tXLA Label: concatenate\n\t\tShape: s32[2,784000,5,5]\n\t\t==========================\n\n\tBuffer 2:\n\t\tSize: 74.77MiB\n\t\tEntry Parameter Subshape: s32[1,784000,5,5]\n\t\t==========================\n\n\tBuffer 3:\n\t\tSize: 74.77MiB\n\t\tEntry Parameter Subshape: s32[1,784000,5,5]\n\t\t==========================\n\n"
     ]
    }
   ],
   "source": [
    "num_edges_removed_bds = 0\n",
    "optimizer_bds = bds(0.0, beta1=FLAGS.beta1, beta2=FLAGS.beta2, eps=FLAGS.eps, graft_eps=0.0, weight_decay=0.0, b=4, transpose=True, graft_type=0)\n",
    "bds_state = optimizer_bds.init(global_grads_list[0])\n",
    "log_det_bds = jax.tree_map(lambda g: jnp.zeros(len(global_grads_list)), global_grads_list[0])\n",
    "eps = FLAGS.eps\n",
    "for i, grads in tqdm(enumerate(global_grads_list)):\n",
    "  updates, new_opt_state = optimizer_bds.update(grads, bds_state, grads)\n",
    "  log_det_bds = jax.tree_map(lambda l, v: l.at[i].set(v), log_det_bds, new_opt_state[0].logdet)\n",
    "  bds_state = new_opt_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d327654e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683ed1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### PLOTTING #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b47461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(p, label=None):\n",
    "  if isinstance(p, dict):\n",
    "    for k, v in p.items():\n",
    "      yield from flatten(v, k if label is None else f\"{label}.{k}\")\n",
    "  else:\n",
    "    yield (label, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba7c163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(name, a, b, summary_writer_true, summary_writer_false):\n",
    "  with summary_writer_true.as_default():\n",
    "    for step, i, in enumerate(a):\n",
    "      tf.summary.scalar(name, i.item(), step=step)\n",
    "  with summary_writer_false.as_default():\n",
    "    for step, i, in enumerate(b):\n",
    "      tf.summary.scalar(name, i.item(), step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2de31c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir logs\n",
    "# !rm -rf logs\n",
    "log_det_tds_overall = jnp.zeros(len(global_grads_list))\n",
    "log_det_bds_overall = jnp.zeros(len(global_grads_list))\n",
    "import tensorflow as tf\n",
    "# current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = 'logs/tds'\n",
    "summary_writer_tds = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "# current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = 'logs/bds'\n",
    "summary_writer_bds = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "try:\n",
    "  log_det_tds = dict(flatten(log_det_tds.unfreeze()))\n",
    "except:\n",
    "  log_det_tds = dict(flatten(log_det_tds))\n",
    "try:\n",
    "  log_det_bds = dict(flatten(log_det_bds.unfreeze()))\n",
    "except:\n",
    "  log_det_bds = dict(flatten(log_det_bds))\n",
    "for k, v in log_det_tds.items():\n",
    "  plot(k, log_det_tds[k], log_det_bds[k], summary_writer_tds, summary_writer_bds)\n",
    "  log_det_tds_overall+=log_det_tds[k]\n",
    "  log_det_bds_overall+=log_det_bds[k]\n",
    "\n",
    "plot(\"overall_logdet\", log_det_tds_overall, log_det_bds_overall, summary_writer_tds, summary_writer_bds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c46381",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb7b1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5b6970",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir logs --port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557f7097",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill 198880"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a294eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d661908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tds_state[0].num_edges_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca85cd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for b=1\n",
    "bds_state[0].num_edges_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fd5d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af692d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60d3ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for b=4\n",
    "bds_state[0].num_edges_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df34fd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
